---
import Layout from '../../layouts/Layout.astro';
import Header from '../../components/Header.astro';
import TableOfContents from '../../components/TableOfContents.astro';
import AuthorsSidebar from '../../components/AuthorsSidebar.astro';

const authors = [
	{
		name: 'Justin Wang',
		title: 'Co-Founder & Principal Researcher',
		linkedin: 'https://www.linkedin.com/in/jstwng/'
	}
];
---

<Layout title="Our Self-Destroying Species" description="Novana examines the techniques that computer scientists have developed to progress towards AGI and argue for a multilateral, public-private cooperation framework on AI governance and alignment to be considered in the case that AGI does come to fruition.">
	<Header currentPath="/research" />
	
	<main class="main">
		<nav class="breadcrumb">
			<div class="container-narrow">
				<a href="/research" class="breadcrumb-link">
					<svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
						<path d="M19 12H5M12 19l-7-7 7-7"/>
					</svg>
					Back to Research
				</a>
			</div>
		</nav>

		<article class="article">
			<div class="container-narrow">
				<header class="article-header">
					<div class="article-tags">
						<span class="tag">AI</span>
						<span class="tag">Miscellaneous</span>
						<span class="tag">Advocacy</span>
					</div>
					
					<h1 class="article-title">Our Self-Destroying Species</h1>
					
					<p class="article-subtitle">Novana examines the techniques that computer scientists have developed to progress towards AGI and argues for a multilateral, public-private cooperation framework on AI governance and alignment to be considered in the case that AGI does come to fruition.</p>
					
					<div class="article-meta">
						<div class="authors">
							<svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
								<path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"></path>
								<circle cx="12" cy="7" r="4"></circle>
							</svg>
							<span>Justin Wang</span>
						</div>
						<div class="article-date">
							<svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
								<rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
								<line x1="16" y1="2" x2="16" y2="6"></line>
								<line x1="8" y1="2" x2="8" y2="6"></line>
								<line x1="3" y1="10" x2="21" y2="10"></line>
							</svg>
							<span>June 2025</span>
						</div>
						<div class="reading-time">
							<span>20 min read</span>
						</div>
					</div>
				</header>

				<div class="article-content">
					<section class="abstract">
						<h2 id="abstract">Abstract</h2>
						
						<p>The recent, rapid development of artificial intelligence (AI) has yielded impressive results: leading models have proven proficiency in solving olympiad-level mathematics questions, passing the law school admissions test (more commonly known as the LSAT), and producing one-shot responses to complex programming tasks. However, as researchers begin to ponder whether or not their inventions will soon evolve into artificial general intelligence (AGI), many have shifted their sights to the future: industry leaders, academics, and policy makers have begun to increasingly voice their concerns about the potential species-wide risks that come with the development of a superintelligent AI system. In this paper, I examine the techniques that computer scientists have developed to progress towards AGI and argue for a multilateral, public-private cooperation framework on AI governance and alignment to be considered in the case that AGI does come to fruition.</p>
						
					</section>

					<section class="content-section">
						<h2 id="introduction">Introduction</h2>
						<p>"Can machines think?" posed by Alan Turing in his 1950 seminal paper "Computing Machinery and Intelligence," remains a foundational question that researchers to-date continue to ponder and challenge (Turing 433). Undoubtedly, artificial intelligence (AI) has made leaps and strides in terms of its capabilities – leading models have proven proficiency in solving olympiad-level mathematics questions, passing the law school admissions test (more commonly known as the LSAT), and producing one-shot responses to complex programming tasks (Stanford Institute for Human-Centered Artificial Intelligence). Nonetheless, the answer to the very question Alan Turing asked 75 years ago remains unclear. The line between artificial intelligence and artificial general intelligence (AGI), AI that possesses human-level thinking abilities, is something that computer scientists and philosophers are not sure about; where modern-day AI lands with regards to its ability to produce genuine thought is widely debated.</p>

						<p>One of the criteria for thought is reasoning. Therefore, many benchmarks have emerged to gauge AI systems' ability to reason. This, in and of itself, poses a significant question to researchers: what does it mean to be able to reason? Some researchers report that AI models have been tested on complex tasks and seem to – on a surface level, at least – possess sophisticated reasoning. Others, on the other hand, have shown that there are many simple problems that AI fails at that humans could solve intuitively (Barassi). Inconsistency regarding AI's ability to problem solve has sparked intense debate about whether AI models are able to truly reason, or rather, if they are merely able to probabilistically produce outputs that resemble the result of sophisticated reasoning.</p>

						<p>The difference between these two possibilities is of utmost importance. True reasoning systems are a prerequisite for AGI. Current systems, however, lack many key indicators of true reasoning capabilities (Ma et al.). Probabilistic models often struggle with tasks that require genuine cause-and-effect understanding, counterfactual thinking, or reasoning about situations that lie outside of their training data (Stępka et al.). Discrepancies between AI reasoning abilities and their performance on benchmarks have led researchers to explore increasingly unique mathematical and computational approaches to develop new models capable of reasoning. From breakthroughs in deep learning, to reinforcement learning, to probabilistic methods, AI research developments are taking place at breakneck speed and may soon provide a means to synthetically produce reasoning systems.</p>

						<p>Closely related to AI innovation are discussions about AI's ramifications on human society— an increasing number of industry leaders, academics, and policy makers have raised public concerns about the risks of misaligned AGI. The Paperclip Maximizer thought experiment, for example, proposes that tasking AI with even a simple task, such as maximizing paperclip production, may result in the total destruction of humanity (Bostrom). What may seem like such a trivial scenario underlies the overarching concern held by those advocating for AI alignment: that objective-pursuing systems pose a significant, existential risk to human values and survival.</p>

						<p>In this paper, I research the mathematical and computational methods that researchers have used to progress towards AGI and argue for multilateral, public-private cooperation on AI governance and alignment in the case that aforementioned technology does eventually arise. Through examining current AI model performance – strengths, weaknesses, and gaps – as well as unboxing the approaches that researchers are using to develop improved AI reasoning systems, we can begin to understand if we are witnessing the rise of AGI and establish appropriate policy frameworks so as to protect human society.</p>
					</section>

					<section class="content-section">
						<h2 id="understanding-agi">Understanding AGI</h2>
						<p>Developing AGI is a major research goal within the AI field, with companies like OpenAI, Anthropic, Google DeepMind, and DeepSeek racing to do so first (Kumpulainen, Terziyan). AGI refers to a subtype of AI – one that matches and surpasses human capabilities across all reasoning tasks (Xu). In contrast to "narrow" AI that specialize in specific domains such as image recognition, natural language processing, or chess, a hypothesized AGI system would possess the computational flexibility and general intelligence to complete any cognitive task that humans can accomplish. More tangibly, this means that AGI would be able to possess abstract reasoning, creative problem solving capabilities, emotional understanding, and domain-agnostic knowledge.</p>

						<p>The distinction between AI and AGI is incredibly important to understanding AGI's impacts on society if developed. Though today's leading AI systems are able to accomplish impressive cognitive tasks, they remain fundamentally narrow in scope. Deep Blue was able to defeat chess champion Garry Kasparov in a six-game match, but only because IBM trained the model on a supercomputer that was specifically optimized for learning chess (Greenemeier). Large language models are able to conduct advanced natural language processing, but lack the autonomous reasoning that would characterize true AGI. AGI differs from these models in that it would be able to achieve human-level performance on all domains, rather than just in specialized tasks.</p>

                        <p>The ramifications of AGI's generalizable intelligence would be groundbreaking. An AGI system would be able to revolutionize innovation by: applying interdisciplinary knowledge that specialist human researchers may miss; accelerate development through non-stop, scalable research efforts; and recursively improve itself so as to continuously improve its computational capabilities. Optimists believe that AGI could beneficially affect mankind in ways parallelling the three Industrial Revolutions, which saw the invention of steam power, electricity, and the internet, respectively (Marr). Pessimists, on the other hand, see AGI as a force that would easily fall out of human control and have a high likelihood of producing extinction-level threats to humanity (Kokotaljo et al.).</p>
					</section>

					<section class="content-section">
						<h2 id="defining-reasoning">Defining Reasoning</h2>
						<p>To begin to understand the extent to which researchers have progressed towards AGI – and if its development is even possible – we must first examine how well current AI models are able to reason; to examine how well current AI models are able to reason, we must understand what reasoning entails. Academic literature broadly defines reasoning as an umbrella of intentional cognitive processes that is directed toward forming conclusions based on a contextual base of information (Kyllonen).</p>

						<p>From there, different scholars further subdivide reasoning into constituent categories. Hačatrjana and Namsone, two education researchers, outline two primary scholarly schools of thought: verbal reasoning, mathematical and/or quantitative reasoning, and visual-spatial and/or non-verbal reasoning; and inductive, deductive, abductive, and analogical reasoning (Hačatrjana and Namsone). Verbal reasoning comprises mastery of vocabulary, grammar, propositional analysis, and discourse comprehension so as to form conclusions through language. Mathematical and quantitative reasoning is the analysis of numerical and quantitative relationships, with models, data, or equations serving as context. Visual-spatial and non-verbal reasoning focuses on the interpretation of visual and spatial information without language. More tangibly speaking, this subcategory of reasoning includes navigation and abstract pattern recognition. Deductive reasoning is a logical framework that starts with assumed true statements and ends with derived conclusions. Inductive reasoning builds off of patterns between disparate observations so as to generalize to a broader conclusion. Abductive reasoning differs from deductive and inductive reasoning in that it often grapples with incomplete, ambiguous evidence. It focuses on forming hypotheses and identifying the conclusion that is most likely to be true. Analogical reasoning focuses on finding structural parallels between different situations, and applying truths from one situation to form conclusions about another related situation.</p>

						<p>Given the plethora of capabilities that the "reasoning" umbrella encompasses, researchers have created a diverse variety of assessments that have been created to gauge AI models' ability to reason. The Massive Multitask Language Understanding (MMLU) benchmark, for example, is a recurring framework of evaluation that is designed to assess how well AI models are able to reason with natural language (Hendrycks et al.). There are also sets of benchmarks that attempt to gauge mathematical and quantitative reasoning. As AI models become increasingly strong in this domain, however, evaluations have become more difficult. The Stanford AI Index Report 2025 states, "… AI systems are approaching near-perfect performance on benchmarks like GSM8K and MATH, which primarily assess high school and college-level mathematics. To push the boundaries further, researchers have voiced a need for benchmarks that test truly advanced mathematics, including problems in number theory, real analysis, algebraic geometry, and category theory." (Stanford Institute for Human-Centered Artificial Intelligence) The Mathematical Olympiad qualifying exam is the new gold standard for this reasoning subcategory; OpenAI's GPT-1o model is currently able to score 74.4% there. In addition to natural language and quantitative reasoning benchmarks, researchers have also conceived many visual-spatial reasoning assessments, including iVISPAR, VSR, Open3DVQA, and SpatialEval. Collectively, these tests span tasks such as object recognition, scene comprehension and navigation, and inter-object relationship identification (Stanford Institute for Human-Centered Artificial Intelligence). Their formats vary dramatically, though: iVISPAR takes the form of a sliding tile puzzle, whereas Open3DVQA pulls thousands of samples from an "efficient semi-automated tool in a high-fidelity urban simulator." (Mayer et al.)</p>
					</section>

					<section class="content-section">
						<h2 id="current-ai-performance">Where AI Models Stand Today</h2>
						<p>How leading AI models are performing on these benchmarks is tracked by researchers worldwide. According to the Artificial Intelligence Index Report 2025, AI model capabilities are scaling "faster than ever." Model performance on the MMLU has been increasing steadily for the past few years, with average accuracy rising from ~30% in 2019 to over 92% in 2024. The highest recorded score on the MMLU was posted in late 2024 by OpenAI's o1-preview model. A more difficult verbal reasoning benchmark was created in 2024, however. The report states, "…a team of researchers from the University of Toronto, University of Waterloo, and Carnegie Mellon introduced MMLU-Pro, a more challenging variant of MMLU." Model performance on this benchmark is slightly lower, with OpenAI's o1-mini model scoring 80.30% and DeepSeek-R1 scoring 84%. AI performance in verbal reasoning, according to performance on the MMLU, seems to have exhibited a near-linear progression, with room for improvement with the new challenges posed by MMLU-Pro. Performance on mathematical and quantitative reasoning benchmarks seem to be exhibiting a different trend: model performance on GSM8K has seemed to have saturated, with 2024 accuracy being 97.72%. The same is the case with the MATH benchmark; performance reached 97.90% in 2025, compared to 90% in 2024 and approximately 80% in 2023. However, as previously iterated, researchers have tried to address saturation on mathematical and quantitative reasoning benchmarks with the development of much more difficult assessments. Performance on FrontierMath, for example, is exceptionally low— as of 2024, OpenAI's o3 model solved only 25.20% of questions correctly. The next best models are almost incapable of solving any FrontierMath problems: Gemini 1.5 Pro, Claude 3.5 Sonnet, o1-preview, GPT-4o, and Grok 2 Beta scored 2%, 2%, 1%, 1%, and 0%, respectively. With regards to visual-spatial and non-verbal reasoning, AI performance on the VCR benchmark finally improved to the human baseline level. (Stanford Institute for Human-Centered Artificial Intelligence)</p>
					</section>

					<section class="content-section">
						<h2 id="driving-innovations">Three Driving Innovations</h2>
						<p>To understand how researchers have been able to produce AI models that are capable of scoring so highly across natural language, mathematical reasoning, and visual-spatial benchmarks, it is worthwhile to examine the methods that have driven disproportionate performance improvements over time.</p>

						<p>First and foremost is deep learning. Its foundations for deep learning were laid with the first neural network, which was proposed by neuroscientists Warren McCulloch and Walter Pitts in their 1943 piece "A Logical Calculus of the Ideas Immanent in Nervous Activity." (McCulloch and Pitts 115) Subsequent innovation slowed down during a series of AI winters in the 1970s and 1980s, but a revitalization of neural networks, driven by improved computing power, took place in the 2010s. Ever since, deep learning algorithms powered by multi-layer neural networks have formed an important basis for modern AI progress, creating breakthroughs in pattern recognition and forecasting. Starting in the early 2010s, researchers began to train multi-layer neural networks on large datasets, which revolutionized AI capabilities in vision and speech (LeCun et al.). During this time, deep learning drove dramatic improvements in pattern recognition accuracy: in 2011, the DanNet neural network surpassed human performance on visual pattern recognition; and in 2012, the landmark AlexNet model demonstrated neural networks' superiority with regards to previous methods on tasks in image recognition. Later developments like ResNets allowed researchers to train ultra-deep networks, which further boosted performance in domains like natural language processing (He et al.). Deep learning methods have demonstrated groundbreaking capabilities as researchers have scaled compute and training, and are likely to contribute significantly to the development of AGI. Deep learning is probabilistic, however, which intrinsically produces limitations with regards to innate reasoning. For this reason, it is clear that deep learning alone will not be able to power AGI.</p>

						<p>Reinforcement learning methods have built upon models that possess the strengths associated with deep learning and have enabled AI models to top human benchmarks in several domains through a trial-and-error learning style. The way in which reinforcement learning is designed teaches AI models to maximize external reward by extracting insights from a context environment (Sutton et al.). Deep reinforcement learning combines neural networks with reinforcement learning methods, and has allowed for AI to become capable at complex reasoning tasks that were previously out-of-reach with deep learning methods in isolation. DeepMind's 2016 DQN, for example, used deep reinforcement learning to create an AI model that was able to play dozens of Atari video games at an expert level (Silver). DQN was succeeded by a series of headline-grabbing achievements, also from DeepMind. AlphaGo, released in 2016, had mastered the game of Go, which was previously considered an AI-hard challenge that was significantly more complex than that of chess (Sedol). AlphaZero, released the following year, generalized the approach that AlphaGo pioneered, extending superhuman game-playing abilities to chess and shogi through self-play training (Silver et al.). AlphaStar, released in 2019, achieved "Grandmaster" status in the video game Starcraft II (The AlphaStar Team). The common denominator between DQN, AlphaGo, AlphaZero, and AlphaStar was that they used deep learning to approximate value functions and reinforcement learning to generate general decision-making strategies with complex games. Reinforcement learning builds off of deep learning well, allowing AI to learn how to achieve rewards in unstructured environments, and is likely another technique that will drive AGI development.</p>

						<p>Deep reinforcement learning AI models typically excel only in one domain at once, which means that its capabilities are not generalizable to all cognitive tasks. Alongside deep learning and reinforcement learning, probabilistic methods have contributed to AI development in that they have improved upon models' generality and robustness. Probabilistic graphical models like Bayesian networks and probabilistic programming have allowed AI models to adjust to uncertainty, perform inference, and use prior knowledge as context (Russell and Norvig). Probabilistic models have seen significant progress in building upon deep reinforcement learning in recent years, with Bayesian deep learning and variational autoencoders serving as two examples of major landmark innovations. The former quantifies model uncertainty, while the latter introduces latent-variable probabilistic models for content generation. Tangibly speaking, this means that probabilistic models inject calibrated uncertainty and principled decision-making capabilities into deep reinforcement models, which are usually confident even when they produce incorrect outputs. Combining the capabilities of neural networks and probabilistic models has, in and of itself, spawned its own field of study; neuro-symbolic AI is an incredibly active research area that may play a role in shaping AGI development (Bhuyan et al.).</p>
					</section>

					<section class="content-section">
						<h2 id="challenges-to-development">Challenges to Development</h2>
						<p>In light of these major AI innovations, there are significant hurdles that stand in the way of future development, leaving researchers unclear as to whether or not AGI can ever truly be developed. Two major areas that may pose significant contributors to constrained innovation are hardware limitations and overbearing policies.</p>

						<p>The "scaling hypothesis" contends that more computational resources correlates to improved AI model performance (Rushing and Gomez-Lavin). As such, governments and corporations are exploring larger training datasets, larger model sizes, and increased computational resources. These efforts are epitomized by OpenAI, Oracle, and Softbank's Stargate Project, which is estimated to invest $500 billion in AI infrastructure – namely, data centers – through 2029 (OpenAI). As AI models scale – in dataset size, model size, and computational resources – energy and hardware resources are likely to become an increasingly larger bottleneck in progress. Training and running AI models necessitates the usage of immense electrical power; dedicated data centers powering large graphics processing units (GPUs) and tensor processing units (TCUs) have scaled to levels such that they now consume as much energy as small cities. For example, according to the study by the Wharton School of Business, OpenAI's GPT-3 consumed approximately 1,287 MWh of electricity for training— an amount that could power ~120 U.S. homes for an entire year (Walther). Even relatively efficient models, like China's DeepSeek, may require grid upgrades and climate-infrastructure updates for continued improvement. And, though training may require significant amounts of electrical power, deployment for millions of users may be even more energy intensive (Lizogat). The computations done by AI models strain electrical grids, but also cooling infrastructure. For every kWh, approximately 2 liters of water are needed for cooling (Nicoletti et al.).</p>

						<p>The raw materials that drive AI also prove to be significant constraining factors. The GPUs and GPUs required to run the computers on which AI models are trained require rare-earth metals like cobalt and lithium to produce (SFA Oxford). China has approximately 50-80% of all stakes in global production, which has increased global dependence upon East Asian supply chains for AI development.</p>

						<p>Beyond hardware, AI development will be largely shaped by public policy resulting from societal trust in the technology. Here, a significant challenge lies in managing public skepticism with regards to AI risk. Between 49% and 52% of United States and United Kingdom survey respondents have a negative sentiment with regards to AI, believing that its risks outweigh its benefits. In the United Kingdom, 87% of respondents supported safety certification before model deployment and 60% supported an outright ban on superintelligent models. American polls show similar results: 71% of U.S. voters believe that AI risks outweigh AI benefits (Dreksler et al.). About half of all AI researchers themselves are fearful that the probability that AI will cause human extinction is above 10% (Egan). The widespread, negative sentiment surrounding that of AI is similar to past backlash that met large-scale, disruptive technologies: over a century ago, early automobiles resulted in the passing of Red Flag laws, which required people to walk in front of cars with red flags; similarly, the invention of the Internet in the 1990s sparked calls from U.S. policymakers for strict regulation (Kyprianou). In hindsight, these reactions were dramatic and are remembered in history as overreactions to innovation. A similar reaction may be spurred from policymakers with AI development, causing overcorrections in AI regulation such startup bans or conservative emergency policies. These could result in overconstrained policy frameworks, reducing the rate at which researchers are able to train and deploy their models, proving as great a challenge as hardware constraints.</p>
					</section>

					<section class="content-section">
						<h2 id="p-doom">P(doom)</h2>
						<p>Despite these challenges, narrow AI models have continued to beat human benchmarks on an increasing number of domain areas. As performance increases, many experts have now shifted their focus to the future, with industry leaders, academics, and policy makers having publicly voiced their concerns about the risks that come with the development of AGI. Foremost is the fear of humans losing control of a superintelligent AI, leading to P(doom)— a now ubiquitous term that captures any given person's estimate that AI will result in the extinction of humanity (Rainey). Longtime AI safety advocate Eliezer Yudkowsky, who has spent "two decades warning that powerful AI systems could, and likely will, kill all of humanity," now puts his estimate of the probability that AI causes human extinction at 99% (Henshall). The "Godfather" of AI, Geoffrey Hinton, upon resigning from Google Brain, stated that there is between a 10% and 20% chance of human extinction from AI within the next 30 years, pointing out that we have "never confronted something more intelligent than ourselves" and therefore may not be able to control superintelligent AI (Milmo). "AI 2027" which was written by a team led by an ex-OpenAI governance researcher, has been circulating in the news and on social media for the past few weeks; in it, a hyper realistic timeline is outlined in which superintelligent AI overtakes and destroys the human race within the next 3 years (Kokotajlo et al.).</p>
					</section>

					<section class="content-section">
						<h2 id="humanity-next-steps">Humanity's Next Steps</h2>
						<p>I believe that confronting the existential risk that comes with the development of AGI requires unprecedented, global cooperation. AGI's potentially detrimental impacts will not be constrained to national borders, and there needs to be multilateral, public-private cooperation on AI governance and alignment, especially from the U.S., European Union, and China. In contrast to prior technological races – like the Cold War's Space Race – the downside risks that come with AGI are so severe that a purely competitive approach would result in a prisoner's dilemma in which all of humanity faces a significantly increased chance of extinction. To improve the probability that humankind is able to survive its own creation, I believe that we need to establish a global framework for cooperation resembling those that our governments have used for other existential threats like nuclear weapons, pandemics, and climate change.</p>

						<p>An imperative starting point for AI governance is the establishment of international agreements that set shared safeguards on AI development, parallelling the Nuclear Non-Proliferation Treaty and its oversight body, the International Atomic Energy Agency. The idea of the establishment of such an agency has already been put forth by UN Secretary-General António Guterres and is backed by leading AI companies. This agency would "place restrictions on AI deployment, vet compliance with safety standards, and track the computing power usage," of notable AI projects with the goal of monitoring the transparent development of superintelligent AI projects. In a similar vein, the United Kingdom hosted a Global AI Safety Summit in 2023 and proposed an international AI risk research body. Historical precedent demonstrates that even the fiercest geopolitical rivals have the capability to cooperate when faced with greater, mutual threats: the U.S. and Soviet Union notably negotiated arms control treaties so as to prevent the onset of a global, nuclear war. Similarly, nations today must agree to mutual, transparent oversight over the development of all superintelligent AI projects in an effort to more comprehensively monitor progress and manage risks as they arise.</p>

						<p>In conjunction with multilateral cooperation, public-private alignment is also crucial. The most advanced models have always been led by private companies and academic labs. As such, governments alone are unable to play a significant role in dictating AI development. In 2023, seven leading AI firms – including OpenAI, Google, Meta, Amazon, and Microsoft – agreed to voluntary commitments negotiated by the White House, pledging to take steps like: pre-release safety training; transparency with regards to AI vulnerabilities; investing in cybersecurity for AI; and implementing watermarks into AI-generated content. I believe, however, that voluntary steps are far too weak a link between the public and private sectors. Binding standards backed by legal requirements would prove to be a much more effective means of ensuring that all major players in the AI development space will respect safety practices and should be a topic seriously considered by policymakers.</p>
					</section>

					<section class="content-section">
						<h2 id="conclusion">Conclusion</h2>
						<p>The pursuit of developing AGI is a defining technological challenge of our 21st century. AI models have demonstrated incredible capabilities – solving olympiad-level mathematics questions, passing the law school admissions test, and producing one-shot responses to complex programming tasks – but currently lack the generality robustness that would define a truly general AGI system. The advent of AGI would allow AI models to effectively complete all cognitive tasks that humans are able to complete, and hold the potential to revolutionize technological innovation as we know it. However, the stakes accompanying a transition from AI to AGI are qualitatively different from those of any other prior technological revolution. In contrast to the steam engine, electricity, or the Internet, AGI represents the potential creation of something that is more intelligent than we, its creators. While experts debate about the likelihood of AGI being invented, or the probability that AGI will result in human extinction, a clear path forward requires us as a species to preemptively react to a world in which potentially misaligned, superintelligent AI might set out on a mission to destroy our species to accomplish its broader goals. The multilateral, public-private cooperation proposal put forth here is just the beginning of what needs to be a global effort to safeguard mankind's future survival.</p>

						<p>Ultimately, the question posed by Alan Turing in 1950 – "Can machines think?" – may soon receive a definitive answer. Whether that answer marks humanity's greatest invention or its own demise will depend not on our technical capabilities alone, but more importantly, our collective cooperation and shared belief in that the AI system that we engineer must be fundamentally aligned with human values. The stakes could not be higher, and the time for action could not be more urgent. In the global race toward AGI, we must make sure that we do not leave behind the very human species we originally sought to advance.</p>
					</section>

					<section class="references">
						<h2 id="works-cited">Works Cited</h2>
						<div class="reference-list">
							<div class="reference">
								<span class="ref-number">1.</span>
								<span class="ref-text">Turing, Alan M. "Computing Machinery and Intelligence." <em>Mind</em>, vol. 59, no. 236, 1950, pp. 433–460.</span>
							</div>
							<div class="reference">
								<span class="ref-number">2.</span>
								<span class="ref-text">Greenemeier, Larry. "20 Years After Deep Blue: How AI Has Advanced Since Conquering Chess." <em>Scientific American</em>, 9 May 2024.</span>
							</div>
							<div class="reference">
								<span class="ref-number">3.</span>
								<span class="ref-text">Stanford HAI. <em>AI Index Report 2025</em>. Stanford Institute for Human‑Centered Artificial Intelligence, 2025.</span>
							</div>
							<div class="reference">
								<span class="ref-number">4.</span>
								<span class="ref-text">Ma, Yuxi, Chi Zhang, and Song-Chun Zhu. "Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models." <em>arXiv</em>, 2023, arXiv:2307.03762.</span>
							</div>
							<div class="reference">
								<span class="ref-number">5.</span>
								<span class="ref-text">Bostrom, Nick. <em>Superintelligence: Paths, Dangers, Strategies</em>. Oxford University Press, 2014.</span>
							</div>
							<div class="reference">
								<span class="ref-number">6.</span>
								<span class="ref-text">Walther, Cornelia C. "The Hidden Cost of AI: Energy Consumption." <em>Knowledge@Wharton</em>, University of Pennsylvania.</span>
							</div>
							<div class="reference">
								<span class="ref-number">7.</span>
								<span class="ref-text">Stępka, Ignacy, Mateusz Lango, and Jerzy Stefanowski. "Frontiers in Generative AI: Survey and New Models." <em>arXiv</em>, 2024, arXiv:2408.04842v1.</span>
							</div>
							<div class="reference">
								<span class="ref-number">8.</span>
								<span class="ref-text">Kumpulainen, Samu, and Vagan Terziyan. "Industry 4.0 Applications and Use Cases in Smart Manufacturing." <em>Procedia Computer Science</em>, vol. 221, 2022.</span>
							</div>
							<div class="reference">
								<span class="ref-number">9.</span>
								<span class="ref-text">Xu, Bowen. "Toward Trustworthy AI: The Future of Alignment." <em>arXiv</em>, 2024, arXiv:2404.10731v1.</span>
							</div>
							<div class="reference">
								<span class="ref-number">10.</span>
								<span class="ref-text">Marr, Bernard. "AI Overhyped: Fantasy or Truly the Next Industrial Revolution?" <em>BernardMarr.com</em>, 12 June 2024.</span>
							</div>
							<div class="reference">
								<span class="ref-number">11.</span>
								<span class="ref-text">Kokotajlo, Daniel, Eli Lifland, Thomas Larsen, Romeo Dean, and Scott Alexander. "AI 2027." <em>AI‑2027.com</em>.</span>
							</div>
							<div class="reference">
								<span class="ref-number">12.</span>
								<span class="ref-text">Hačatrjana, Žaneta, and Maija Namsone. "Verbal Reasoning and Its Application in Educational Practice." <em>Oxford Research Encyclopedia of Education</em>, Oxford University Press, 2024.</span>
							</div>
							<div class="reference">
								<span class="ref-number">13.</span>
								<span class="ref-text">Hendrycks, Dan, et al. "Measuring Massive Multitask Language Understanding." <em>arXiv</em>, 2025, arXiv:2502.03214.</span>
							</div>
							<div class="reference">
								<span class="ref-number">14.</span>
								<span class="ref-text">LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep Learning." <em>Nature</em>, vol. 521, no. 7553, 2015, pp. 436–444.</span>
							</div>
							<div class="reference">
								<span class="ref-number">15.</span>
								<span class="ref-text">Sutton, Richard S., and Andrew G. Barto. <em>Reinforcement Learning: An Introduction</em>. 2nd ed., MIT Press, 2018.</span>
							</div>
						</div>
					</section>
				</div>

				<footer class="article-footer">
					<div class="footer-content">
						<div class="copyright">
							<p>© 2025 Novana. All rights reserved.</p>
							<p>This analysis is intended for institutional investors and qualified professionals.</p>
						</div>
						<div class="footer-actions">
							<button class="btn btn-outline">
								<svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
									<path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"/>
									<polyline points="16,6 12,2 8,6"/>
									<line x1="12" y1="2" x2="12" y2="15"/>
								</svg>
								Share
							</button>
							<button class="btn">
								<svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
									<path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/>
									<polyline points="7,10 12,15 17,10"/>
									<line x1="12" y1="15" x2="12" y2="3"/>
								</svg>
								Download PDF
							</button>
						</div>
					</div>
				</footer>
			</div>
		</article>
	</main>
	
	<TableOfContents />
	<AuthorsSidebar authors={authors} />
</Layout>

<style>
	.main {
		min-height: calc(100vh - 70px);
	}

	.breadcrumb {
		padding: 20px 0;
		border-bottom: 1px solid #e5e5e5;
	}

	.breadcrumb-link {
		display: inline-flex;
		align-items: center;
		gap: 8px;
		color: #666;
		text-decoration: none;
		font-weight: 500;
		transition: color 0.2s ease;
	}

	.breadcrumb-link:hover {
		color: #6C43D7;
	}

	.article {
		padding: 60px 0 80px;
	}

	.article-header {
		margin-bottom: 60px;
		padding-bottom: 40px;
		border-bottom: 1px solid #e5e5e5;
	}

	.article-tags {
		display: flex;
		flex-wrap: wrap;
		gap: 8px;
		margin-bottom: 24px;
	}

	.article-title {
		font-size: clamp(2rem, 4vw, 2.8rem);
		font-weight: 700;
		color: #1a1a1a;
		margin-bottom: 24px;
		line-height: 1.1;
	}

	.article-subtitle {
		font-size: 20px;
		color: #666;
		line-height: 1.5;
		margin-bottom: 32px;
		font-style: italic;
	}

	.article-meta {
		display: flex;
		align-items: center;
		gap: 24px;
		color: #888;
		font-size: 14px;
	}

	.authors,
	.article-date {
		display: flex;
		align-items: center;
		gap: 6px;
	}

	.reading-time {
		margin-left: auto;
		font-weight: 500;
		color: #6C43D7;
	}

	.article-content {
		font-size: 18px;
		line-height: 1.7;
		color: #333;
	}

	.article-content h2 {
		font-size: 1.5rem;
		font-weight: 600;
		margin-top: 48px;
		margin-bottom: 24px;
		color: #1a1a1a;
	}

	/* Add extra space above the Results header */
	.article-content h2#results-header {
		margin-top: 64px;
	}

	.abstract {
		background: #f8f9fa;
		border: 1px solid #e5e5e5;
		border-radius: 12px;
		padding: 40px;
		margin-bottom: 48px;
	}

	.abstract h2 {
		font-size: 20px;
		font-weight: 600;
		margin-bottom: 24px;
		color: #1a1a1a;
		letter-spacing: 0.5px;
	}

	.abstract p {
		line-height: 1.6;
		margin: 0;
	}

	.content-section {
		margin-bottom: 40px;
	}

	.content-section p {
		margin-bottom: 20px;
	}

	.references {
		margin-top: 60px;
		padding-top: 40px;
		border-top: 1px solid #e5e5e5;
	}

	.references h2 {
		font-size: 24px;
		font-weight: 600;
		margin-bottom: 32px;
		color: #1a1a1a;
	}

	.reference-list {
		font-size: 16px;
		line-height: 1.6;
	}

	.reference {
		display: flex;
		margin-bottom: 16px;
		gap: 12px;
	}

	.ref-number {
		color: #1a1a1a;
		font-weight: 500;
		flex-shrink: 0;
	}

	.ref-text {
		color: #666;
	}

	.ref-text em {
		font-style: italic;
	}

	.article-footer {
		margin-top: 60px;
		padding-top: 40px;
		border-top: 1px solid #e5e5e5;
	}

	.footer-content {
		display: flex;
		justify-content: space-between;
		align-items: flex-end;
		gap: 32px;
	}

	.copyright {
		color: #888;
		font-size: 14px;
		line-height: 1.5;
	}

	.footer-actions {
		display: flex;
		gap: 16px;
	}

	@media (max-width: 768px) {
		.article {
			padding: 40px 0 60px;
		}

		.article-meta {
			flex-wrap: wrap;
			gap: 16px;
		}

		.reading-time {
			margin-left: 0;
		}

		.abstract {
			padding: 24px;
		}

		.footer-content {
			flex-direction: column;
			align-items: flex-start;
			gap: 24px;
		}

		.footer-actions {
			width: 100%;
			justify-content: center;
		}
	}
</style>